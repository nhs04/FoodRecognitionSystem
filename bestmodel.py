# -*- coding: utf-8 -*-
"""bestModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GcCUW0wPYkWKWmkuCvHj8FSNTR_BeaM3

# **III. Convolutional neural networks**
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import os
import numpy as np
import shutil
import tensorflow as tf
import pickle as pkl
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping

# Data Preparation and Preprocessing
data_dir = "/content/drive/MyDrive/data"


# Check the number of images in each category
for i in range(1, 10):
    path = os.path.join(data_dir, str(i))
    print(f'Category {i} has {len(os.listdir(path))} images.')

# Function to identify and remove corrupted images
def verify_images(folder_path):
    for fname in os.listdir(folder_path):
        fpath = os.path.join(folder_path, fname)
        try:
            img = tf.io.read_file(fpath)
            img = tf.io.decode_image(img)
        except:
            print(f'Removing corrupted image: {fpath}')
            os.remove(fpath)

# Apply the cleaning function to each category
for i in range(1, 10):
    verify_images(os.path.join(data_dir, str(i)))

# Create directories for train, validation, and test sets
sets = ['train', 'val', 'test']
for s in sets:
    set_path = os.path.join(data_dir, s)
    if not os.path.exists(set_path):
        os.makedirs(set_path)
    for i in range(1, 10):
        class_path = os.path.join(set_path, str(i))
        if not os.path.exists(class_path):
            os.makedirs(class_path)

# Split the data into train, validation, and test folders
def split_data(source, dest_train, dest_val, dest_test, split_train=0.8, split_val=0.1):
    files = os.listdir(source)
    np.random.shuffle(files)
    train_idx = int(len(files) * split_train)
    val_idx = int(len(files) * (split_train + split_val))
    for file in files[:train_idx]:
        shutil.copy(os.path.join(source, file), os.path.join(dest_train, file))
    for file in files[train_idx:val_idx]:
        shutil.copy(os.path.join(source, file), os.path.join(dest_val, file))
    for file in files[val_idx:]:
        shutil.copy(os.path.join(source, file), os.path.join(dest_test, file))

# Apply the splitting function
for i in range(1, 10):
    src_folder = os.path.join(data_dir, str(i))
    train_folder = os.path.join(data_dir, 'train', str(i))
    val_folder = os.path.join(data_dir, 'val', str(i))
    test_folder = os.path.join(data_dir, 'test', str(i))
    split_data(src_folder, train_folder, val_folder, test_folder)

# Define the architecture of the CNN model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),  # 32 filters of size 3x3, ReLU activation, input shape of (150, 150, 3)
    MaxPooling2D(2, 2),  # Max pooling with pool size of 2x2
    Conv2D(64, (3,3), activation='relu'),  # 64 filters of size 3x3, ReLU activation
    MaxPooling2D(2, 2),  # Max pooling with pool size of 2x2
    Conv2D(128, (3,3), activation='relu'),  # 128 filters of size 3x3, ReLU activation
    MaxPooling2D(2, 2),  # Max pooling with pool size of 2x2
    Flatten(),  # Flatten the output for dense layers
    Dense(512, activation='relu'),  # Fully connected layer with 512 neurons, ReLU activation
    Dropout(0.5),  # Dropout layer with dropout rate of 0.5
    Dense(9, activation='softmax')  # Output layer with 9 neurons for classification, softmax activation
])


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Set up image data generators for training and validation data
train_datagen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to the range [0, 1] for training data
val_datagen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to the range [0, 1] for validation data

# Flow training images in batches of 32 using train_datagen generator
train_generator = train_datagen.flow_from_directory(
    os.path.join(data_dir, 'train'),  # Path to the training data directory
    target_size=(150, 150),  # Resize images to 150x150
    batch_size=32,  # Batch size of 32
    class_mode='categorical'  # Use categorical labels
)

# Flow validation images in batches of 32 using val_datagen generator
val_generator = val_datagen.flow_from_directory(
    os.path.join(data_dir, 'val'),  # Path to the validation data directory
    target_size=(150, 150),  # Resize images to 150x150
    batch_size=32,  # Batch size of 32
    class_mode='categorical'  # Use categorical labels
)


# Set up Early Stopping callback to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10)  # Monitor validation loss and stop training if it doesn't improve for 10 epochs


# Calculate steps per epoch and validation steps
steps_per_epoch = len(train_generator.filenames) // train_generator.batch_size  # Dynamically adjust steps per epoch based on training data size and batch size
validation_steps = len(val_generator.filenames) // val_generator.batch_size  # Dynamically adjust validation steps based on validation data size and batch size


# Train the model
history = model.fit(
    train_generator,  # Training data generator
    steps_per_epoch=steps_per_epoch,  # Dynamically adjusted steps per epoch
    epochs=50,  # Train for 50 epochs
    validation_data=val_generator,  # Validation data generator
    validation_steps=validation_steps,  # Dynamically adjusted validation steps
    callbacks=[early_stopping]  # Early stopping callback
)
model_path = os.path.join('/content/drive/MyDrive', 'best_model.pkl')
pickle.dump(best_model, open(model_path, 'wb'))

# Evaluate the model on the test set
test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
    os.path.join(data_dir, 'test'),
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'
)

test_loss, test_acc = model.evaluate(test_generator)
print(f'Test accuracy: {test_acc}')